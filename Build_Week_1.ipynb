{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=get(\"https://www.goodreads.com/list/show/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "request=url.text\n",
    "soup_data=Soup(request,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#url list\n",
    "urls=soup_data.findAll(class_=\"bookTitle\")\n",
    "url_list=[]\n",
    "url_list=[\"https://www.goodreads.com\"+str(list(str(url).split(\" \"))[2])[6::] for url in urls]\n",
    "\n",
    "len(url_list)\n",
    "#url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#title list\n",
    "titles=soup_data.findAll(class_=\"bookTitle\")\n",
    "title_list=[]\n",
    "for title in range(len(titles)):\n",
    "    title_list.append(titles[title].text.strip())\n",
    "len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#authors list\n",
    "authors=soup_data.findAll(class_=\"authorName\")\n",
    "author_list=[]\n",
    "for author in range(len(authors)):\n",
    "    author_list.append(authors[author].text)\n",
    "len(author_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of ratings\n",
    "ratings_list=[]\n",
    "for urls in url_list:\n",
    "    url_link=get(urls)\n",
    "    request_rating=url_link.text\n",
    "    soup_data_rating=Soup(request_rating,\"html.parser\")\n",
    "    ratings=soup_data_rating.findAll(\"div\",{\"id\":\"bookMeta\"})\n",
    "    try:\n",
    "        ratings_list.append(int(ratings[0].find(\"a\",{\"class\":\"gr-hyperlink\"}).text.strip().split(\"\\n\")[0].replace(\",\",\"\")))\n",
    "    except:\n",
    "        ratings_list.append(np.nan)\n",
    "print(len(ratings_list))\n",
    "#ratings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#number of reviews\n",
    "reviews_list=[]\n",
    "for urls in url_list:\n",
    "    url_link=get(urls)\n",
    "    request_review=url_link.text\n",
    "    soup_data_review=Soup(request_review,\"html.parser\")\n",
    "    reviews=soup_data_review.findAll(\"div\",{\"id\":\"bookMeta\"})\n",
    "    try:\n",
    "        reviews_list.append(int(reviews[0].text.strip().split(\"\\n\")[-2].strip().replace(\",\",\"\")))\n",
    "    except:\n",
    "        reviews_list.append(np.nan)\n",
    "print(len(reviews_list))\n",
    "#reviews_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#avg rating\n",
    "avg_ratings=soup_data.findAll(class_=\"minirating\")\n",
    "avg_list=[]\n",
    "for avg in range(len(avg_ratings)):\n",
    "    try:\n",
    "        avg_list.append(round(float(avg_ratings[avg].text[0:4].strip())))\n",
    "    except:\n",
    "        avg_list.append(np.nan)\n",
    "print(len(avg_list))\n",
    "#avg_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[759,\n",
       " 374,\n",
       " 652,\n",
       " 734,\n",
       " 870,\n",
       " 552,\n",
       " 500,\n",
       " 371,\n",
       " 464,\n",
       " 372,\n",
       " 391,\n",
       " 501,\n",
       " 335,\n",
       " 465,\n",
       " 460,\n",
       " 489,\n",
       " 372,\n",
       " 241,\n",
       " 226,\n",
       " 302,\n",
       " 529,\n",
       " 4100,\n",
       " 423,\n",
       " 377,\n",
       " nan,\n",
       " 756,\n",
       " 351,\n",
       " 629,\n",
       " 487,\n",
       " 736,\n",
       " 635,\n",
       " 563,\n",
       " 619,\n",
       " 485,\n",
       " 503,\n",
       " 196,\n",
       " 326,\n",
       " 221,\n",
       " 288,\n",
       " 291,\n",
       " 368,\n",
       " 662,\n",
       " 406,\n",
       " 318,\n",
       " 639,\n",
       " nan,\n",
       " 661,\n",
       " 503,\n",
       " 272,\n",
       " 1177,\n",
       " 288,\n",
       " 704,\n",
       " 566,\n",
       " 509,\n",
       " 425,\n",
       " 332,\n",
       " 162,\n",
       " 307,\n",
       " 1006,\n",
       " 320,\n",
       " 440,\n",
       " 467,\n",
       " 276,\n",
       " 206,\n",
       " 349,\n",
       " 401,\n",
       " 335,\n",
       " 291,\n",
       " 153,\n",
       " 321,\n",
       " 389,\n",
       " 653,\n",
       " nan,\n",
       " 309,\n",
       " 467,\n",
       " 637,\n",
       " 1014,\n",
       " 485,\n",
       " 294,\n",
       " 541,\n",
       " 450,\n",
       " 255,\n",
       " 294,\n",
       " 457,\n",
       " 512,\n",
       " 936,\n",
       " 710,\n",
       " 483,\n",
       " 566,\n",
       " 448,\n",
       " 403,\n",
       " 296,\n",
       " 399,\n",
       " 381,\n",
       " 751,\n",
       " 387,\n",
       " 432,\n",
       " 304,\n",
       " 279,\n",
       " 475]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num of pages\n",
    "pages_list=[]\n",
    "for urls in url_list:\n",
    "    url_link=get(urls)\n",
    "    request_pages=url_link.text\n",
    "    soup_data_pages=Soup(request_pages,\"html.parser\")\n",
    "    num_pages=soup_data_pages.findAll(\"span\",{\"itemprop\":\"numberOfPages\"})\n",
    "    try:\n",
    "        pages_list.append(int(num_pages[0].text.split(\" \")[0]))\n",
    "    except:\n",
    "        pages_list.append(np.nan)\n",
    "print(len(pages_list))\n",
    "#pages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#published year\n",
    "\n",
    "published_list=[]\n",
    "for urls in url_list:\n",
    "    url_link=get(urls)\n",
    "    request_published=url_link.text\n",
    "    soup_data_published=Soup(request_published,\"html.parser\")\n",
    "    published_year=soup_data_published.findAll(\"div\",{\"id\":\"details\"})\n",
    "    try:\n",
    "        published_list.append(published_year[0].text.split(\"\\n\")[4][-4::])\n",
    "    except:\n",
    "        published_list.append(np.nan)\n",
    "print(len(published_list))\n",
    "#published_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#series\n",
    "series_list=[]\n",
    "for urls in url_list:\n",
    "    url_link=get(urls)\n",
    "    request_series=url_link.text\n",
    "    soup_data_series=Soup(request_series,\"html.parser\")\n",
    "    series=soup_data_series.findAll(\"h2\",{\"id\":\"bookSeries\"})\n",
    "    try:\n",
    "        if series[0].text.strip()==\"\":\n",
    "            series_list.append(0)\n",
    "        else:\n",
    "            series_list.append(1)\n",
    "    except:\n",
    "        series_list.append(np.nan)\n",
    "print(len(series_list))\n",
    "#series_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#genres\n",
    "genres_list=[]\n",
    "genres_lists=[]\n",
    "for urls in url_list:\n",
    "    url_link=get(urls)\n",
    "    request_genres=url_link.text\n",
    "    soup_data_genres=Soup(request_genres,\"html.parser\")\n",
    "    genres=soup_data_genres.findAll(class_=\"rightContainer\")\n",
    "    try:\n",
    "        for i in range(1,10,3):\n",
    "            genres_lists.append(genres[0].text.strip().split(\"Genres\")[1].split(\"\\n\\n\\n\")[i].split(\"\\n\")[-1].strip())\n",
    "        genres_list.append(genres_lists[0:3])\n",
    "    except:\n",
    "        genres_list.append(np.nan)\n",
    "print(len(genres_list))\n",
    "#genres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#awards\n",
    "awards_list=[]\n",
    "for urls in url_list:\n",
    "    url_link=get(urls)\n",
    "    request_awards=url_link.text\n",
    "    soup_data_awards=Soup(request_awards,\"html.parser\")\n",
    "    awards=soup_data_awards.findAll(class_=\"award\")\n",
    "    try:\n",
    "        awards_lists=[award.text.strip() for award in awards]\n",
    "        awards_list.append(awards_lists)\n",
    "    except:\n",
    "        awards_list.append(np.nan)\n",
    "print(len(awards_list))\n",
    "#awards_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['London, England', '(United Kingdom)'],\n",
       "  ['Hogwarts School of Witchcraft and Wizardry', '(United Kingdom)'],\n",
       "  ['Pagford', '(United Kingdom)']],\n",
       " [['District 12, Panem', 'Capitol, Panem', 'Panem', '(United States)']],\n",
       " [['Hogwarts School of Witchcraft and Wizardry', '(United Kingdom)'],\n",
       "  ['England', '(United Kingdom)']],\n",
       " [['Hogwarts School of Witchcraft and Wizardry,\\n1994', '(United Kingdom)'],\n",
       "  ['Scotland', 'United Kingdom']],\n",
       " [['Hogwarts School of Witchcraft and Wizardry', '(United Kingdom)'],\n",
       "  ['London, England', '(United Kingdom)']],\n",
       " [['Molching,\\n1939', '(Germany)'], ['Germany']],\n",
       " [['Chicago, Illinois', '(United States)'], ['Illinois', '(United States)']],\n",
       " [['Kabul', '(Afghanistan)'],\n",
       "  ['Fremont, California', '(United States)'],\n",
       "  ['Afghanistan', '…more\\nPeshawar', '(Pakistan)'],\n",
       "  ['…less']],\n",
       " [['Jackson, Mississippi,\\n1962', '(United States)'],\n",
       "  ['Jackson, Mississippi', '(United States)'],\n",
       "  ['Mississippi', '(United States)']],\n",
       " [['Afghanistan', 'Kabul', '(Afghanistan)'], ['Herat', '(Afghanistan)']],\n",
       " [['District 12, Panem', 'Capitol, Panem', 'Panem', '(United States)']],\n",
       " [['Forks, Washington', '(United States)'],\n",
       "  ['Phoenix, Arizona', '(United States)'],\n",
       "  ['Washington (state)', '(United States)']],\n",
       " [['United States of America', 'Joliet, Illinois,\\n1932', '(United States)'],\n",
       "  ['Ithaca, New York,\\n1932', '(United States)']],\n",
       " [['Stockholm', '(Sweden)'],\n",
       "  ['Hedeby Island', '(Sweden)'],\n",
       "  ['Hedestad, Stockholm', '(Sweden)'],\n",
       "  ['…more\\nSweden', '…less']],\n",
       " [['Pacific Ocean', 'Pondicherry', '(India)'],\n",
       "  ['Toronto, Ontario', '(Canada)'],\n",
       "  ['…more\\nTomatlan', '(Mexico)'],\n",
       "  ['…less']],\n",
       " [['Paris', '(France)'],\n",
       "  ['London, England', '(United Kingdom)'],\n",
       "  ['France', '…more\\nEngland', '(United Kingdom)'],\n",
       "  ['United Kingdom', '…less']],\n",
       " [['Philadelphia, Pennsylvania,\\n1973', '(United States)'],\n",
       "  ['Pennsylvania,\\n1973', '(United States)'],\n",
       "  ['United States of America,\\n1973']],\n",
       " [['United States of America']],\n",
       " [['Swindon, England,\\n1999', '(United Kingdom)'],\n",
       "  ['London, England,\\n1999', '(United Kingdom)'],\n",
       "  ['England,\\n1999', '(United Kingdom)']],\n",
       " [['United States of America', 'South Carolina', '(United States)'],\n",
       "  ['Tiburon, South Carolina,\\n1964', '(United States)']],\n",
       " [['Bursa', '(Turkey)'],\n",
       "  ['Izmir', '(Turkey)'],\n",
       "  ['Detroit, Michigan', '(United States)'],\n",
       "  ['…more\\nBerlin', '(Germany)'],\n",
       "  ['…less']],\n",
       " nan,\n",
       " [['United States of America', 'Providence, Rhode Island', '(United States)'],\n",
       "  ['Rhode Island', '(United States)']],\n",
       " [['New York City, New York', '(United States)'],\n",
       "  ['Montauk, New York', '(United States)'],\n",
       "  ['New Jersey', '(United States)'],\n",
       "  ['…more\\nSt. Louis, Missouri', '(United States)'],\n",
       "  ['Las Vegas, Nevada', '(United States)'],\n",
       "  ['Long Island, New York', '(United States)'],\n",
       "  ['…less']],\n",
       " [['New York City, New York', '(United States)'],\n",
       "  ['West Virginia', '(United States)']],\n",
       " [['Forks, Washington', '(United States)'],\n",
       "  ['Washington (state)', '(United States)']],\n",
       " [['England', '(United Kingdom)'],\n",
       "  ['France', 'London, England', '(United Kingdom)'],\n",
       "  ['…more\\nUnited Kingdom', '…less']],\n",
       " [['Forks, Washington', '(United States)'],\n",
       "  ['Washington (state)', '(United States)'],\n",
       "  ['La Push, Washington', '(United States)']],\n",
       " [['Spain', 'Barcelona, Catalonia', '(Spain)']],\n",
       " [['Rome', '(Italy)'], ['Vatican City', '(Italy)'], ['Italy']],\n",
       " [['United States of America', 'Chicago, Illinois', '(United States)'],\n",
       "  ['Chattanooga, Tennessee', '(United States)'],\n",
       "  ['…more\\nMinnesota', '(United States)'],\n",
       "  ['Wisconsin', '(United States)'],\n",
       "  ['Illinois', '(United States)'],\n",
       "  ['…less']],\n",
       " [['Forks, Washington', '(United States)'],\n",
       "  ['La Push, Washington', '(United States)'],\n",
       "  ['Volterra', '(Italy)'],\n",
       "  ['…more\\nWashington (state)', '(United States)'],\n",
       "  ['…less']],\n",
       " [['Arizona', '(United States)'],\n",
       "  [''],\n",
       "  ['', 'Other Editions (183)'],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  ['All Editions', ' | ', 'Add a New Edition', ' | ', 'Combine']],\n",
       " [['New York City, New York,\\n2007', '(United States)']],\n",
       " [['Stockholm', '(Sweden)'], ['Sweden']],\n",
       " [['United States of America']],\n",
       " [['New York City, New York,\\n2003', '(United States)'],\n",
       "  ['Dresden,\\n1945', '(Germany)']],\n",
       " [['Alabama', '(United States)']],\n",
       " [['Hailsham', '(United Kingdom)'],\n",
       "  ['Norfolk, England', 'The Cottages', '(United Kingdom)'],\n",
       "  ['…more\\nKingsfield', '(United Kingdom)'],\n",
       "  ['…less']],\n",
       " [['United Kingdom,\\n1946', '    \\nGuernsey,\\n1946']],\n",
       " [['Italy', 'India', 'Bali', '(Indonesia)']],\n",
       " nan,\n",
       " [['Angelfield', '(United Kingdom)']],\n",
       " nan,\n",
       " [['Brooklyn, New York City, New York', '(United States)'],\n",
       "  ['Prague (Praha)', '(Czech Republic)'],\n",
       "  ['New York State', '(United States)'],\n",
       "  ['…more\\nNew York City, New York', '(United States)'],\n",
       "  ['Flatbush, Brooklyn, New York', '(United States)'],\n",
       "  ['…less']],\n",
       " [['Chicago, Illinois', '(United States)'], ['Illinois', '(United States)']],\n",
       " [['Westminster Palace, London, England,\\n1521', '(United Kingdom)'],\n",
       "  ['England', '(United Kingdom)']],\n",
       " [['Alagaësia']],\n",
       " [['Paris', '(France)'],\n",
       "  ['Raleigh, North Carolina', '(United States)'],\n",
       "  ['New York City, New York', '(United States)']],\n",
       " [['Westeros', 'Essos']],\n",
       " [['Hunan', '(China)'], ['China']],\n",
       " [['Europe,\\n1974', '    \\nAmsterdam,\\n1974', '(Netherlands)']],\n",
       " [['Stockholm', '(Sweden)'], ['Gothenburg', '(Sweden)'], ['Sweden']],\n",
       " [['Chatham Islands', '(New Zealand)'],\n",
       "  ['Neerbeke, West Vlaanderen', '(Belgium)'],\n",
       "  ['Buenas Yerbas', '(United States)'],\n",
       "  ['…more\\nLondon, England', '(United Kingdom)'],\n",
       "  ['Seoul, South Korea', '(Korea, Republic of)'],\n",
       "  ['Maui, Hawaii', '(United States)'],\n",
       "  ['…less']],\n",
       " nan,\n",
       " [['Montana', '(United States)'],\n",
       "  [''],\n",
       "  ['', 'Other Editions (75)'],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  ['All Editions', ' | ', 'Add a New Edition', ' | ', 'Combine']],\n",
       " [['England', '(United Kingdom)']],\n",
       " [['England', '(United Kingdom)'],\n",
       "  ['United Kingdom', 'Old Town', '(United Kingdom)']],\n",
       " [['United Kingdom,\\n1806']],\n",
       " nan,\n",
       " [['New Hampshire', '(United States)']],\n",
       " [['Takamatsu', '(Japan)']],\n",
       " [['Ukraine', 'Trochenbrod', '(Ukraine)'],\n",
       "  ['Odessa', '(Ukraine)'],\n",
       "  ['…more\\nLviv', '(Ukraine)'],\n",
       "  ['…less']],\n",
       " nan,\n",
       " [['(Pakistan)']],\n",
       " [['Pittsburgh, Pennsylvania,\\n1970', '(United States)'],\n",
       "  ['Lexington, Kentucky,\\n1964', '(United States)'],\n",
       "  ['Paris,\\n1988', '(France)'],\n",
       "  [''],\n",
       "  ['', 'Other Editions (117)'],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  ['All Editions', ' | ', 'Add a New Edition', ' | ', 'Combine']],\n",
       " [['Dominican Republic', 'Washington Heights, New York', '(United States)'],\n",
       "  ['Paterson, New Jersey', '(United States)'],\n",
       "  ['…more\\nRutgers University, New Brunswick, New Jersey (United States)',\n",
       "   'Santo Domingo',\n",
       "   '(Dominican Republic)'],\n",
       "  ['Baní', '(Dominican Republic)'],\n",
       "  ['Samaná', '(Dominican Republic)'],\n",
       "  ['Santiago, Dominican Republic', '…less']],\n",
       " nan,\n",
       " [['Iran, Islamic Republic of', 'Tehran,\\n1980', '(Iran)'], ['Persia']],\n",
       " [['Seattle, Washington', '(United States)'],\n",
       "  [''],\n",
       "  ['', 'Other Editions (120)'],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  ['All Editions', ' | ', 'Add a New Edition', ' | ', 'Combine']],\n",
       " nan,\n",
       " [['St. Jude, Illinois', '(United States)']],\n",
       " nan,\n",
       " [['United States of America',\n",
       "   'Terrell County, Texas,\\n1980',\n",
       "   '(United States)']],\n",
       " nan,\n",
       " [['Canada,\\n1999', '    \\nToronto, Ontario', '(Canada)'],\n",
       "  ['Southwest Ontario,\\n1914', '(Canada)']],\n",
       " [['Kingsbridge, England', '(United Kingdom)'],\n",
       "  [''],\n",
       "  ['', 'Other Editions (189)'],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  ['All Editions', ' | ', 'Add a New Edition', ' | ', 'Combine']],\n",
       " nan,\n",
       " [['United States of America', 'Oregon', '(United States)']],\n",
       " [['Addis Ababa', '(Ethiopia)'],\n",
       "  ['Madras', '(India)'],\n",
       "  ['Kochi', '(India)'],\n",
       "  ['…more\\nAden', '(Yemen)'],\n",
       "  ['Djibouti', 'Asmara', '(Eritrea)'],\n",
       "  ['Khartoum', '(Sudan)'],\n",
       "  ['Nairobi', '(Kenya)'],\n",
       "  ['New York City, New York', '(United States)'],\n",
       "  ['Boston, Massachusetts', '(United States)'],\n",
       "  ['Rome', '(Italy)'],\n",
       "  ['…less']],\n",
       " nan,\n",
       " [['New York City, New York', '(United States)'], ['Slonim', '(Poland)']],\n",
       " [['Paris,\\n1942', '(France)'],\n",
       "  ['Paris,\\n2002', '(France)'],\n",
       "  [''],\n",
       "  ['', 'Other Editions (146)'],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  ['All Editions', ' | ', 'Add a New Edition', ' | ', 'Combine']],\n",
       " [['United States of America']],\n",
       " nan,\n",
       " [['Bombay', '(India)'],\n",
       "  ['Afghanistan', 'Australia', '…more\\nIndia', '…less']],\n",
       " nan,\n",
       " [['Maine', '(United States)']],\n",
       " [['Mellen', '(United States)'],\n",
       "  ['Wisconsin', '(United States)'],\n",
       "  ['Pusan,\\n1952', '(Korea, Republic of)']],\n",
       " [['Willesden, North London,\\n1974', '(United Kingdom)']],\n",
       " [['United Kingdom',\n",
       "   'India,\\n1895',\n",
       "   '    \\nLondon, England',\n",
       "   '(United Kingdom)']],\n",
       " nan,\n",
       " [['Colorado Springs, Colorado', '(United States)']],\n",
       " [['Manhattan, New York City, New York', '(United States)'],\n",
       "  ['New York City, New York', '(United States)'],\n",
       "  ['New York State', '(United States)']],\n",
       " [['Boston, Massachusetts', '(United States)'],\n",
       "  ['Philadelphia, Pennsylvania', '(United States)']],\n",
       " [['London, England', '(United Kingdom)'], ['Florida', '(United States)']],\n",
       " nan,\n",
       " [['Eyam, Derbyshire, England,\\n1666', '(United Kingdom)']],\n",
       " [['Miami, Florida', '(United States)'],\n",
       "  ['Atlantic Ocean', 'Bermuda Triangle,\\n2006', '    '],\n",
       "  [''],\n",
       "  ['Other Editions (189)'],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  [''],\n",
       "  ['All Editions', ' | ', 'Add a New Edition', ' | ', 'Combine']],\n",
       " [['Sudan', 'Ethiopia', 'Kakuma', '(Kenya)'],\n",
       "  ['…more\\nMarial Bai', '(South Sudan)'],\n",
       "  ['…less']]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_list=[]\n",
    "for urls in url_list:\n",
    "    url_link=get(urls)\n",
    "    request_places=url_link.text\n",
    "    soup_data_places=Soup(request_places,\"html.parser\")\n",
    "    places=soup_data_places.findAll(id=\"bookDataBox\")\n",
    "    try:\n",
    "        pre_list_places=places[0].text.split(\"\\nSetting\")[1].strip().split(\"Literary Awards\")[0].strip().split(\"\\n\\n\\n\")\n",
    "        places_lists=[places.strip().split(\"\\n\\n\") for places in pre_list_places ]\n",
    "        places_list.append(places_lists)\n",
    "    except:\n",
    "        places_list.append(np.nan)\n",
    "print(len(places_list))\n",
    "places_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Number of Ratings</th>\n",
       "      <th>Number of Reviews</th>\n",
       "      <th>Average Ratings</th>\n",
       "      <th>Number of Pages</th>\n",
       "      <th>Published Year</th>\n",
       "      <th>Series</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Awards</th>\n",
       "      <th>Places</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.goodreads.com/book/show/136251.Har...</td>\n",
       "      <td>Harry Potter and the Deathly Hallows (Harry Po...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>2958364</td>\n",
       "      <td>68421</td>\n",
       "      <td>5.0</td>\n",
       "      <td>759.0</td>\n",
       "      <td>2007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction]</td>\n",
       "      <td>[Locus Award Nominee for Best Young Adult Nove...</td>\n",
       "      <td>[[London, England, (United Kingdom)], [Hogwart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "      <td>The Hunger Games (The Hunger Games, #1)</td>\n",
       "      <td>Suzanne Collins</td>\n",
       "      <td>6683135</td>\n",
       "      <td>175476</td>\n",
       "      <td>4.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>2008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction]</td>\n",
       "      <td>[Locus Award Nominee for Best Young Adult Book...</td>\n",
       "      <td>[[District 12, Panem, Capitol, Panem, Panem, (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.goodreads.com/book/show/1.Harry_Po...</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>2574373</td>\n",
       "      <td>41988</td>\n",
       "      <td>4.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction]</td>\n",
       "      <td>[Locus Award Nominee for Best Young Adult Nove...</td>\n",
       "      <td>[[Hogwarts School of Witchcraft and Wizardry, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.goodreads.com/book/show/6.Harry_Po...</td>\n",
       "      <td>Harry Potter and the Goblet of Fire (Harry Pot...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>2748838</td>\n",
       "      <td>49554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>734.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction]</td>\n",
       "      <td>[Hugo Award for Best Novel (2001), Mythopoeic ...</td>\n",
       "      <td>[[Hogwarts School of Witchcraft and Wizardry,\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.goodreads.com/book/show/2.Harry_Po...</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>2652319</td>\n",
       "      <td>45347</td>\n",
       "      <td>4.0</td>\n",
       "      <td>870.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction]</td>\n",
       "      <td>[Bram Stoker Award for Works for Young Readers...</td>\n",
       "      <td>[[Hogwarts School of Witchcraft and Wizardry, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>https://www.goodreads.com/book/show/2744.Anans...</td>\n",
       "      <td>Anansi Boys</td>\n",
       "      <td>Neil Gaiman</td>\n",
       "      <td>202125</td>\n",
       "      <td>10199</td>\n",
       "      <td>4.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction]</td>\n",
       "      <td>[Locus Award for Best Fantasy Novel (2006), My...</td>\n",
       "      <td>[[London, England, (United Kingdom)], [Florida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>https://www.goodreads.com/book/show/16527.Daug...</td>\n",
       "      <td>Daughter of Fortune</td>\n",
       "      <td>Isabel Allende</td>\n",
       "      <td>119426</td>\n",
       "      <td>4158</td>\n",
       "      <td>4.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction]</td>\n",
       "      <td>[Premio de traducción literaria Valle Inclán N...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://www.goodreads.com/book/show/4965.Year_...</td>\n",
       "      <td>Year of Wonders</td>\n",
       "      <td>Geraldine Brooks</td>\n",
       "      <td>146254</td>\n",
       "      <td>10579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction]</td>\n",
       "      <td>[ALA Alex Award (2002)]</td>\n",
       "      <td>[[Eyam, Derbyshire, England,\\n1666, (United Ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>https://www.goodreads.com/book/show/28186.The_...</td>\n",
       "      <td>The Sea of Monsters (Percy Jackson and the Oly...</td>\n",
       "      <td>Rick Riordan</td>\n",
       "      <td>805198</td>\n",
       "      <td>29067</td>\n",
       "      <td>4.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[Miami, Florida, (United States)], [Atlantic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://www.goodreads.com/book/show/4952.What_...</td>\n",
       "      <td>What Is the What</td>\n",
       "      <td>Dave Eggers</td>\n",
       "      <td>80852</td>\n",
       "      <td>7222</td>\n",
       "      <td>4.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Fantasy, Young Adult, Fiction]</td>\n",
       "      <td>[Independent Publisher Book Award (IPPY) for L...</td>\n",
       "      <td>[[Sudan, Ethiopia, Kakuma, (Kenya)], […more\\nM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  \\\n",
       "0   https://www.goodreads.com/book/show/136251.Har...   \n",
       "1   https://www.goodreads.com/book/show/2767052-th...   \n",
       "2   https://www.goodreads.com/book/show/1.Harry_Po...   \n",
       "3   https://www.goodreads.com/book/show/6.Harry_Po...   \n",
       "4   https://www.goodreads.com/book/show/2.Harry_Po...   \n",
       "..                                                ...   \n",
       "95  https://www.goodreads.com/book/show/2744.Anans...   \n",
       "96  https://www.goodreads.com/book/show/16527.Daug...   \n",
       "97  https://www.goodreads.com/book/show/4965.Year_...   \n",
       "98  https://www.goodreads.com/book/show/28186.The_...   \n",
       "99  https://www.goodreads.com/book/show/4952.What_...   \n",
       "\n",
       "                                                Title            Author  \\\n",
       "0   Harry Potter and the Deathly Hallows (Harry Po...      J.K. Rowling   \n",
       "1             The Hunger Games (The Hunger Games, #1)   Suzanne Collins   \n",
       "2   Harry Potter and the Half-Blood Prince (Harry ...      J.K. Rowling   \n",
       "3   Harry Potter and the Goblet of Fire (Harry Pot...      J.K. Rowling   \n",
       "4   Harry Potter and the Order of the Phoenix (Har...      J.K. Rowling   \n",
       "..                                                ...               ...   \n",
       "95                                        Anansi Boys       Neil Gaiman   \n",
       "96                                Daughter of Fortune    Isabel Allende   \n",
       "97                                    Year of Wonders  Geraldine Brooks   \n",
       "98  The Sea of Monsters (Percy Jackson and the Oly...      Rick Riordan   \n",
       "99                                   What Is the What       Dave Eggers   \n",
       "\n",
       "    Number of Ratings  Number of Reviews  Average Ratings  Number of Pages  \\\n",
       "0             2958364              68421              5.0            759.0   \n",
       "1             6683135             175476              4.0            374.0   \n",
       "2             2574373              41988              4.0            652.0   \n",
       "3             2748838              49554              4.0            734.0   \n",
       "4             2652319              45347              4.0            870.0   \n",
       "..                ...                ...              ...              ...   \n",
       "95             202125              10199              4.0            387.0   \n",
       "96             119426               4158              4.0            432.0   \n",
       "97             146254              10579              4.0            304.0   \n",
       "98             805198              29067              4.0            279.0   \n",
       "99              80852               7222              4.0            475.0   \n",
       "\n",
       "   Published Year  Series                           Genres  \\\n",
       "0            2007     1.0  [Fantasy, Young Adult, Fiction]   \n",
       "1            2008     1.0  [Fantasy, Young Adult, Fiction]   \n",
       "2            2006     1.0  [Fantasy, Young Adult, Fiction]   \n",
       "3            2002     1.0  [Fantasy, Young Adult, Fiction]   \n",
       "4            2004     1.0  [Fantasy, Young Adult, Fiction]   \n",
       "..            ...     ...                              ...   \n",
       "95           2006     1.0  [Fantasy, Young Adult, Fiction]   \n",
       "96           2006     1.0  [Fantasy, Young Adult, Fiction]   \n",
       "97           2002     0.0  [Fantasy, Young Adult, Fiction]   \n",
       "98           2006     1.0  [Fantasy, Young Adult, Fiction]   \n",
       "99           2006     0.0  [Fantasy, Young Adult, Fiction]   \n",
       "\n",
       "                                               Awards  \\\n",
       "0   [Locus Award Nominee for Best Young Adult Nove...   \n",
       "1   [Locus Award Nominee for Best Young Adult Book...   \n",
       "2   [Locus Award Nominee for Best Young Adult Nove...   \n",
       "3   [Hugo Award for Best Novel (2001), Mythopoeic ...   \n",
       "4   [Bram Stoker Award for Works for Young Readers...   \n",
       "..                                                ...   \n",
       "95  [Locus Award for Best Fantasy Novel (2006), My...   \n",
       "96  [Premio de traducción literaria Valle Inclán N...   \n",
       "97                            [ALA Alex Award (2002)]   \n",
       "98                                                 []   \n",
       "99  [Independent Publisher Book Award (IPPY) for L...   \n",
       "\n",
       "                                               Places  \n",
       "0   [[London, England, (United Kingdom)], [Hogwart...  \n",
       "1   [[District 12, Panem, Capitol, Panem, Panem, (...  \n",
       "2   [[Hogwarts School of Witchcraft and Wizardry, ...  \n",
       "3   [[Hogwarts School of Witchcraft and Wizardry,\\...  \n",
       "4   [[Hogwarts School of Witchcraft and Wizardry, ...  \n",
       "..                                                ...  \n",
       "95  [[London, England, (United Kingdom)], [Florida...  \n",
       "96                                                NaN  \n",
       "97  [[Eyam, Derbyshire, England,\\n1666, (United Ki...  \n",
       "98  [[Miami, Florida, (United States)], [Atlantic ...  \n",
       "99  [[Sudan, Ethiopia, Kakuma, (Kenya)], […more\\nM...  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict={\"URL\":url_list,\"Title\":title_list,\"Author\":author_list,\"Number of Ratings\":ratings_list,\"Number of Reviews\":reviews_list,\n",
    "        \"Average Ratings\":avg_list,\"Number of Pages\":pages_list,\"Published Year\":published_list,\n",
    "        \"Series\":series_list,\"Genres\":genres_list,\"Awards\":awards_list,\"Places\":places_list}\n",
    "df=pd.DataFrame(data=df_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    source = requests.get(url).text\n",
    "    soup = BeautifulSoup(source, 'lxml')\n",
    "    return soup\n",
    "\n",
    "\n",
    "url=get(\"https://www.goodreads.com/list/show/5\")\n",
    "def book(url):\n",
    "    soup_data=get_data(url)\n",
    "    \n",
    "    #Creating URL and Titles List\n",
    "    urls_and_titles=soup_data.findAll(class_=\"bookTitle\")\n",
    "    url_list=[\"https://www.goodreads.com\"+str(list(str(url).split(\" \"))[2])[6::] for url in urls_and_titles]\n",
    "    \n",
    "    #Creating Title List\n",
    "    title_list=[title.text.strip() for title in urls_and_titles]\n",
    "    \n",
    "    #Creating Author List\n",
    "    authors=soup_data.findAll(class_=\"authorName\")\n",
    "    author_list=[author.text for author in authors]\n",
    "    \n",
    "    #number of ratings\n",
    "    ratings_list=[]\n",
    "    \n",
    "    #Creating \"for loop\" for iterating through the pages\n",
    "    for urls in url_list:\n",
    "        soup_data=get_data(urls)\n",
    "        \n",
    "        try:\n",
    "            #Creating Ratings List\n",
    "            ratings_list=[]\n",
    "            ratings=soup_data.findAll(\"div\",{\"id\":\"bookMeta\"})\n",
    "            ratings_list.append(int(ratings[0].find(\"a\",{\"class\":\"gr-hyperlink\"}).text.strip().split(\"\\n\")[0].replace(\",\",\"\")))\n",
    "            \n",
    "            #Creating Pages List\n",
    "            num_pages=soup_data.findAll(\"span\",{\"itemprop\":\"numberOfPages\"})\n",
    "            pages_list=[int(page.text.split(\" \")[0]) for page in pages_list]\n",
    "            \n",
    "            #number of reviews\n",
    "            reviews_list=[]\n",
    "            reviews=soup_data.findAll(\"div\",{\"id\":\"bookMeta\"})\n",
    "            reviews_list.append(int(reviews[0].text.strip().split(\"\\n\")[-2].strip().replace(\",\",\"\")))\n",
    "            \n",
    "            #avg rating\n",
    "            avg_ratings=soup_data.findAll(class_=\"minirating\")\n",
    "            avg_list=[]\n",
    "            for avg in range(len(avg_ratings)):\n",
    "                try:\n",
    "                    avg_list.append(round(float(avg_ratings[avg].text[0:4].strip())))\n",
    "                except:\n",
    "                    avg_list.append(np.nan)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "        except:\n",
    "            ratings_list.append(np.nan)\n",
    "            pages_list.append(np.nan)\n",
    "            reviews_list.append(np.nan)\n",
    "            \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
